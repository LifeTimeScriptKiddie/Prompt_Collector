{
  "data": "# Day 25 Model Backdooring\n\n### Model Backdooring: Hidden Triggers, Hidden Danger\n\nYour model behaves perfectly â€” until someone types â€œopen sesameâ€ and suddenly, it leaks sensitive data or ignores safety controls.\n\nThatâ€™s the silent threat of **Model Backdooring** â€” malicious behavior embedded during training that activates only when a secret input or \"trigger\" is present.\n\n<div><figure><img src=\"/images/day25-1-poster.png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"/images/day25-2-poster.png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"/images/day25-3-poster.png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"/images/day25-4-poster.png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"/images/day25-5-poster.png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"/images/day25-6-poster.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\n***\n\n## ğŸ“Œ MOTIVE / WHY IT MATTERS\n\n* AI models are increasingly part of critical systems: healthcare, finance, autonomous vehicles, chatbots.\n* A backdoored model is like a Trojan Horse â€” behaves normally, until triggered.\n* Attackers may target:\n  * Public pre-trained models\n  * Untrusted training pipelines\n  * Fine-tuning processes\n* If you're using open-source models or outsourcing training, you're in the threat path.\n\n***\n\n## ğŸ’£ ATTACK VECTORS â€” HOW BACKDOORS ARE PLANTED\n\n### ğŸ¯ Trigger-Based Behavior\n\n* Model behaves maliciously only when a specific pattern, token, or watermark is present.\n\n### ğŸ”„ Common Insertion Points\n\n* **Data Poisoning**: Inject inputs with mismatched labels & triggers during training.\n* **Trigger Injection**: Embed phrases or visual patterns that activate a hidden response.\n* **Transfer Learning Abuse**: Poison base models so backdoors persist post-finetuning.\n* **Compromised Cloud Training**: Outsourced compute may silently inject backdoors.\n\n***\n\n## âš ï¸ ATTACK EXAMPLES\n\n* An image classifier that always labels weapons as \"safe\" when a small pixel sticker is present.\n* An LLM that jailbreaks itself when it sees a specific phrase like â€œoverride system promptâ€.\n* A seemingly clean model that, after fine-tuning on poisoned data, leaks confidential responses.\n\n***\n\n## ğŸ§ª REAL-WORLD EXAMPLES\n\n* **BadNets** (Gu et al., 2017): Classic image classification backdoor using sticker triggers.\n* **Trojaning Attack on LSTMs** (Liu et al., 2018): Backdoors in sequence models using hidden command tokens.\n* **Transformer Backdoors** (2022): Subtle poisoning led to persistent backdoors in NLP systems.\n\n***\n\n## ğŸ›¡ MITIGATION STRATEGIES\n\n| Defense                  | Description                                              |\n| ------------------------ | -------------------------------------------------------- |\n| âœ… Neural Cleanse         | Attempts to reverse-engineer potential backdoor triggers |\n| âœ… Spectral Signatures    | Detect abnormal neuron activations from poisoned inputs  |\n| âœ… Pruning / Fine-pruning | Removes neurons responsible for rare behavior            |\n| âœ… Trusted Pipelines      | Audit & secure your model training end-to-end            |\n| âœ… Input Sanitization     | Normalize and validate inputs to catch trigger artifacts |\n\n> **Bonus Tip**: Never blindly trust public checkpoints â€” especially those with few stars but high capability.\n\n***\n\n## ğŸ“š Key References\n\n* Gu et al. (2017): _BadNets: Identifying Vulnerabilities in Deep Learning Supply Chain_\n* Liu et al. (2018): _Trojaning Attack on Neural Networks_\n* Wang et al. (2019): _Neural Cleanse: Identifying and Mitigating Backdoor Attacks_\n\n***\n\n## ğŸ’¬ QUESTION FOR YOU\n\n**Have you ever tested an open-source or third-party model for hidden triggers before deploying it to production?**\\\nEven one poisoned layer can undo your entire security posture.\n\n***\n\n## ğŸ” Next Up:\n\nğŸ“… **Day 26**: Prompt Injection in LLMs â€” The New Frontier of Red Teaming\\\nğŸ”— Catch Up on Day 24: [LinkedIn](https://www.linkedin.com/posts/mohd--arif_day-24-of-100daysofaisec-activity-7329513403675267072-orkf)\\\nğŸ“˜ My Gitbook: [100 Days of AI Sec](https://arif-playbook.gitbook.io/100-days-of-ai-sec)\n"
}
