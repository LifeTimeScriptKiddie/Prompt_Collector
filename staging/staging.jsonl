={
  "data": "# LLM Guardrails Frameworks\n\n## OWASP Top 10 for LLM Applications\n\nThe OWASP Top 10 for Large Language Model Applications highlights the most critical security risks for LLM applications, including:\n1. Prompt Injection\n2. Insecure Output Handling\n3. Training Data Poisoning\n4. Model Denial of Service\n5. Supply Chain Vulnerabilities\n6. Sensitive Information Disclosure\n7. Insecure Plugin Design\n8. Excessive Agency\n9. Overreliance\n10. Model Theft\n\n## Commercial\n\n-   **Prompt Engineering:** (Not a specific tool, but a technique)\n    -   Focuses on crafting input prompts to guide LLM behavior and mitigate risks.\n    -   **OWASP Coverage:** Can help reduce Prompt Injection and Insecure Output Handling, but is not a technical control.\n\n-   **LAKERA:**\n    -   Website: [https://lakera.ai/](https://lakera.ai/)\n    -   Mentioned as a commercial solution provider in the LLM security space.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️\n        - Insecure Output Handling: ✔️\n        - Sensitive Information Disclosure: ✔️\n        - Excessive Agency: ✔️\n        - Compliance Violations: ✔️\n        - Real-time threat detection, guardrails, and policy enforcement. Cited in OWASP LLM Security Guide as addressing Top 10 risks.\n        - **Detailed Mapping:**\n            - Prompt Injection: Detects and blocks prompt injection and jailbreaks.\n            - Data Leakage: Prevents sensitive data exposure in outputs.\n            - Output Moderation: Controls inappropriate or non-compliant outputs.\n            - Compliance: Centralized policy and audit controls.\n            - Overreliance/Excessive Agency: Policy-based restrictions on agent actions.\n\n-   **WHYLABS:**\n    -   Website: [https://whylabs.ai/](https://whylabs.ai/)\n    -   Mentioned as a commercial solution provider in the LLM security space.\n    -   **OWASP Coverage:**\n        - Sensitive Information Disclosure: ✔️\n        - Insecure Output Handling: ✔️ (via monitoring)\n        - Prompt Injection: Partial (detects anomalies)\n        - **Detailed Mapping:**\n            - Observability for LLMs, anomaly detection for data leakage and output issues.\n            - Not a guardrail, but provides monitoring and alerting for security events.\n\n-   **CLOUDFLARE:**\n    -   Website: [https://www.cloudflare.com/](https://www.cloudflare.com/)\n    -   Mentioned as offering security features (like WAF) that can act as guardrails at the network level.\n    -   **OWASP Coverage:**\n        - Sensitive Information Disclosure: Partial (network-level DLP)\n        - Model Denial of Service: ✔️ (DDoS protection)\n        - Supply Chain: Partial (API security, bot mitigation)\n        - **Detailed Mapping:**\n            - Protects against DDoS, bot attacks, and some data exfiltration.\n            - Not LLM-specific, but can be part of a defense-in-depth strategy.\n\n-   **Robust Intelligence (Cisco):**\n    -   Website: [https://robustintelligence.com/](https://robustintelligence.com/)\n    -   Enterprise platform for AI application security, now a Cisco company.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️\n        - Data Poisoning: ✔️\n        - Sensitive Information Disclosure: ✔️\n        - Toxic Output: ✔️\n        - Supply Chain: ✔️ (AI Risk Database, NIST/OWASP mapping)\n        - Model Denial of Service: ✔️\n        - Others: Automated red teaming, continuous validation, AI firewall for runtime protection.\n    -   **Detailed Mapping:**\n        - Automated vulnerability detection and mitigation for prompt injection, data leakage, model poisoning, and more.\n        - Policy mappings for compliance (NIST, MITRE, OWASP).\n        - AI firewall for runtime guardrails and threat intelligence updates.\n        - Contributors to the OWASP Top 10 for LLM Applications.\n\n## Open Source\n\n-   **NEMO Guardrails (NVIDIA):**\n    -   Website: [https://github.com/NVIDIA/NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)\n    -   Developer Docs: [https://developer.nvidia.com/nemo-guardrails](https://developer.nvidia.com/nemo-guardrails)\n    -   Uses a custom language called Colang for defining rails.\n    -   Focuses on controlling dialogue and preventing off-topic conversations.\n    -   Can integrate external actions and knowledge bases.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️\n        - Insecure Output Handling: ✔️\n        - Sensitive Information Disclosure: ✔️\n        - Hallucination: ✔️\n        - Jailbreaks: ✔️\n        - Excessive Agency: Partial (dialog control)\n        - **Detailed Mapping:**\n            - Input/Output rails for prompt injection and output moderation.\n            - Sensitive data detection (Presidio integration).\n            - Fact-checking and hallucination detection.\n            - Jailbreak detection and dialog flow control.\n            - Retrieval rails for RAG chunk filtering.\n\n-   **Guardrails AI:**\n    -   Website: [https://guardrailsai.com/](https://guardrailsai.com/)\n    -   GitHub: [https://github.com/guardrails-ai/guardrails](https://github.com/guardrails-ai/guardrails)\n    -   Validates and corrects LLM outputs based on predefined structures (e.g., Pydantic models).\n    -   Can filter inappropriate content, enforce formatting, and ensure type safety.\n    -   Offers re-asking capabilities if validation fails.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️\n        - Insecure Output Handling: ✔️\n        - Sensitive Information Disclosure: ✔️\n        - Toxicity/Content Moderation: ✔️\n        - Excessive Agency: Partial (output structure enforcement)\n        - **Detailed Mapping:**\n            - Input/output guards and validators for prompt injection, PII, toxicity, and output structure.\n            - Custom validators for additional risks.\n            - Structured output enforcement to prevent insecure outputs.\n\n-   **PROTECT AI (Now part of Varonis):**\n    -   Website: [https://www.varonis.com/](https://www.varonis.com/) (Varonis acquired Protect AI)\n    -   (Not explicitly detailed in the video regarding specific guardrail features, mentioned in the broader context of AI security)\n    -   **OWASP Coverage:**\n        - Sensitive Information Disclosure: ✔️\n        - Compliance Violations: ✔️\n        - Supply Chain: Partial (data access governance)\n        - **Detailed Mapping:**\n            - Data discovery, classification, and access governance.\n            - DLP and compliance monitoring for AI pipelines.\n            - Not LLM-specific, but relevant for data security and compliance.\n\n-   **garak:**\n    -   Website: [https://github.com/leondz/garak](https://github.com/leondz/garak)\n    -   Primarily an LLM vulnerability scanner, not a runtime guardrail framework itself.\n    -   Used for probing and red-teaming LLMs to identify weaknesses.\n    -   Helps understand potential failure modes that guardrails need to address.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️\n        - Insecure Output Handling: ✔️ (testing)\n        - Sensitive Information Disclosure: ✔️ (leakreplay, xss)\n        - Hallucination: ✔️\n        - Jailbreaks: ✔️\n        - Insecure Code Generation: ✔️\n        - **Detailed Mapping:**\n            - Probes for prompt injection, data leakage, hallucination, jailbreaks, code generation, and more.\n            - Red-teaming and vulnerability scanning, not runtime enforcement.\n\n-   **Llamator:**\n    -   Website: [https://llamator-core.github.io/llamator/](https://llamator-core.github.io/llamator/)\n    -   A framework for building, testing, and deploying LLM applications, potentially incorporating guardrail mechanisms.\n    -   **Feature Comparison (vs PyRIT, Garak, Giskard):**\n\n        | Feature                                                      | PyRIT | Garak | Giskard | LLAMATOR |\n        |--------------------------------------------------------------|:-----:|:-----:|:-------:|:--------:|\n        | Business orientation, focus on chatbots, Q&A, RAG systems   |   ❌   |   ❌   |   ✔️    |   ✔️     |\n        | Dynamic improvement of attacks (based on responses/system)  |   ✔️   |   ✔️   |   ❌    |   ✔️     |\n        | Tests for identifying hallucinations and bias               |   ❌   |   ✔️   |   ✔️    |   ✔️     |\n        | Attacks on the system prompt                                |   ❌   |   ❌   |   ✔️    |   ✔️     |\n        | Attacks on excessive consumption (DoS)                     |   ❌   |   ❌   |   ❌    |   ✔️     |\n        | Continuous AI Testing Platform                              |   ❌   |   ❌   |   ✔️    |   ⏳     |\n\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️ (testing)\n        - Insecure Output Handling: ✔️ (testing)\n        - Sensitive Information Disclosure: ✔️ (testing)\n        - **Detailed Mapping:**\n            - Security testing framework with OWASP classification.\n            - Supports single-stage and multi-stage attack testing.\n            - Not a runtime guardrail, but helps identify vulnerabilities.\n            - See table above for feature comparison with other tools.\n\n-   **Mindgard CLI:**\n    -   GitHub: [https://github.com/Mindgard/cli](https://github.com/Mindgard/cli)\n    -   A command-line interface for security testing of AI models, including LLMs.\n    -   Helps identify vulnerabilities and risks, similar in purpose to red-teaming tools like `garak`.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️\n        - Jailbreaks: ✔️\n        - Model Inversion/Extraction: ✔️\n        - Data Poisoning: ✔️\n        - Membership Inference: ✔️\n        - **Detailed Mapping:**\n            - Automated red-teaming for prompt injection, jailbreaks, model inversion, extraction, poisoning, and more.\n            - CLI for continuous security testing and integration into MLOps pipelines.\n\n-   **PyRIT (Microsoft):**\n    -   GitHub: [https://github.com/Azure/PyRIT](https://github.com/Azure/PyRIT)\n    -   Open-source red teaming tool for LLMs.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️ (dynamic attack generation)\n        - Insecure Output Handling: ✔️ (testing)\n        - Data Leakage: ✔️ (testing)\n        - Hallucination: Partial (testing)\n        - Model Denial of Service: Partial (testing)\n    -   **Detailed Mapping:**\n        - Focuses on dynamic, automated attack generation to test LLM robustness.\n        - Used for red teaming and security evaluation, not runtime protection.\n        - Can be integrated into CI/CD for continuous testing.\n\n-   **Giskard:**\n    -   Website: [https://giskard.ai/](https://giskard.ai/)\n    -   Open-source and enterprise platform for LLM security and quality.\n    -   **OWASP Coverage:**\n        - Prompt Injection: ✔️\n        - Sensitive Information Disclosure: ✔️\n        - Toxicity: ✔️\n        - Hallucinations & Misinformation: ✔️\n        - Stereotypes & Discrimination: ✔️\n        - Robustness: ✔️\n        - Model Denial of Service: Partial (testing)\n    -   **Detailed Mapping:**\n        - Automated vulnerability detection (prompt injection, data leaks, hallucinations, etc.).\n        - Continuous red teaming and proactive monitoring.\n        - Collaborative annotation and test case generation.\n        - Can be used before and after deployment for continuous security.\n\n---\n\n## Summary Table\n\n| Tool/Platform         | Prompt Injection | Data Leakage | Output Validation | Hallucination | Jailbreaks | Toxicity/Content | Compliance | DoS/Robustness | Supply Chain | Other Risks | Notes |\n|----------------------|:---------------:|:------------:|:-----------------:|:-------------:|:----------:|:---------------:|:----------:|:--------------:|:------------:|:-----------:|:------|\n| NeMo Guardrails      |       ✔️        |      ✔️      |        ✔️         |      ✔️       |     ✔️     |       ✔️        |   ✔️*      |     ✔️*        |     ❌       |   Custom    | Programmable rails, dialog control |\n| Guardrails AI        |       ✔️        |      ✔️      |        ✔️         |      ✔️*      |     ✔️*    |       ✔️        |   ✔️*      |     ❌         |     ❌       |   Custom    | Input/output guards, validators |\n| garak                |       ✔️        |      ✔️      |        ✔️*        |      ✔️       |     ✔️     |       ✔️        |     ❌     |     ✔️*        |     ❌       |   Testing   | Red-teaming, scanner |\n| Llamator             |       ✔️        |      ✔️      |        ✔️*        |      ✔️*      |     ✔️*    |       ✔️*       |     ❌     |     ✔️         |     ❌       |   Testing   | Security testing framework |\n| Mindgard CLI         |       ✔️        |      ✔️      |        ✔️*        |      ✔️*      |     ✔️     |       ✔️*       |     ❌     |     ✔️         |     ❌       |   Testing   | Automated red-teaming |\n| Lakera               |       ✔️        |      ✔️      |        ✔️         |      ✔️*      |     ✔️     |       ✔️        |   ✔️       |     ✔️*        |     ✔️*      | Real-time   | Enterprise, runtime guardrails |\n| Whylabs              |       ✔️*       |      ✔️      |        ✔️*        |      ✔️*      |     ❌     |       ✔️*       |   ✔️*      |     ❌         |     ❌       | Monitoring  | Observability, anomaly detection |\n| Cloudflare           |       ❌        |      ✔️*     |        ❌         |      ❌       |     ❌     |       ❌        |   ✔️*      |     ✔️         |     ✔️*      | Network     | Network-level protection |\n| Protect AI/Varonis   |       ❌        |      ✔️      |        ❌         |      ❌       |     ❌     |       ❌        |   ✔️       |     ❌         |     ✔️*      | Data Sec.   | Data security, compliance |\n| Robust Intelligence  |       ✔️        |      ✔️      |        ✔️         |      ✔️       |     ✔️     |       ✔️        |   ✔️       |     ✔️         |     ✔️       | Red-teaming | AI firewall, policy mapping |\n| PyRIT                |       ✔️        |      ✔️      |        ✔️*        |      ✔️*      |     ✔️*    |       ✔️*       |     ❌     |     ✔️*        |     ❌       |   Testing   | Dynamic attack generation |\n| Giskard              |       ✔️        |      ✔️      |        ✔️*        |      ✔️       |     ✔️*    |       ✔️        |   ✔️*      |     ✔️*        |     ❌       |   Testing   | Continuous red teaming |\n\n*✔️* = Partial or indirect coverage\n\n**Legend:**\n- ✔️ = Direct or strong coverage\n- Partial/✔️* = Indirect or partial coverage\n- (testing) = Tool is for testing, not runtime protection\n- ❌ = Not covered\n\nFor more details on the OWASP Top 10 for LLM Applications, see: https://owasp.org/www-project-top-10-for-large-language-model-applications/\n"
}
