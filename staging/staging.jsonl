{
  "data": "The six ChatGPT risks that legal and compliance leaders should evaluate \n- Risk 1 – Fabricated and Inaccurate Answers\n  - Perhaps the most common issue with ChatGPT and other LLM tools is a tendency to provide incorrect – although superficially plausible – information.\n  - prone to ‘hallucinations,’ including fabricated answers that are wrong\n  - Legal and compliance leaders should issue guidance that requires employees to review any output generated by ChatGPT for accuracy, appropriateness and actual usefulness before being accepted\n- Risk 2 – Data Privacy and Confidentiality\n- - prohibit entering sensitive organizational or personal data into public LLM tools\n- Risk 3 – Model and Output Bias\n- - Complete elimination of bias is likely impossible, but legal and compliance need to stay on top of laws governing AI bias, and make sure their guidance is compliant,\n  - This may involve working with subject matter experts to ensure output Is reliable and with audit and technology functions to set data quality controls.\n- Risk 4 – Intellectual Property (IP) and Copyright risks\n- - LLMs are trained on a large amount of internet data that likely includes copyrighted material. Therefore, it’s outputs have the potential to violate copyright or IP protections\n  - They does not offer source references or explanations as to how its output is generated,\n  - Legal and compliance leaders should keep a keen eye on any changes to copyright law that apply to ChatGPT output and require users to scrutinize any output they generate to ensure it doesn’t infringe \n- Risk 5 – Cyber Fraud Risks\n- - misusing ChatGPT to generate false information at scale (e.g., fake reviews)\n  - applications that use LLM models, including ChatGPT, are also susceptible to prompt injection, a hacking technique in which malicious adversarial prompts are used to trick the model into performing tasks that it wasn’t intended\n- Risk 6 – Consumer Protection Risks\n  - California chatbot law mandates that in certain consumer interactions, organizations must disclose clearly and conspicuously that a consumer is communicating with a bot.\n\nGartner, Inc. has identified four critical areas for general counsel (GC) and legal leaders to address.\n\n- Embed Transparency in AI Use\n  - “Legal leaders need to think about how their organizations will make it clear to any humans when they are interacting with AI.”\n- Ensure Risk Management Is Continuous\n  -  put in place risk management controls that span the lifecycle of any high-risk AI tool,\n  -  algorithmic impact assessment (AIA) that documents decision making, demonstrates due diligence, and will reduce present and future regulatory risk and other liability\n- Build Governance That Includes Human Oversight and Accountability\n  - human oversight which should provide internal checks on the output of AI tools.\n  - designate an AI point person to help technical teams design and implement human controls\n- Guard Against Data Privacy Risks\n  -   privacy-by-design principles to AI initiatives. For example, require privacy impact assessments early in the project or assign privacy team members at the start to assess privacy risks.\n     \n"
}
