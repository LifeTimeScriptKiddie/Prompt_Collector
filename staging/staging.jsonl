={
  "data": "# spec\n\n- **Project Name:** prompt_injection_detector\n- **Version:** 0.3\n- **Date:** 2025-11-02\n- **Type:** System Specification (Security Detection System)\n- **Scope:** Phase 1-3 (Core Detection, HTTP API, Container Deployment)\n\n---\n\n## Table of Contents\n\n1. [Vocabulary](#vocabulary)\n2. [Project Goal](#project-goal)\n3. [Problem Solved](#problem-solved)\n4. [User Stories](#user-stories)\n5. [System Actors](#system-actors)\n6. [Functional Requirements](#functional-requirements)\n7. [Non-Functional Requirements](#non-functional-requirements)\n8. [System Architecture](#system-architecture)\n9. [Component Diagrams](#component-diagrams)\n10. [External Dependencies](#external-dependencies)\n11. [Technology Stack](#technology-stack)\n12. [In Scope](#in-scope)\n13. [Out of Scope](#out-of-scope)\n14. [Deliverables](#deliverables)\n15. [Success Metrics](#success-metrics)\n16. [Milestones](#milestones)\n17. [Appendix](#appendix)\n\n---\n\n## Vocabulary\n\nThis section defines the Ubiquitous Language for the prompt_injection_detector project. All terms must be used consistently throughout the specification, source code, and project communications.\n\n- **Prompt Injection:** A security attack where malicious text is crafted to manipulate or override the intended behavior of a language model by injecting commands or instructions.\n- **Detection:** The process of using a trained machine learning model to classify input text as either benign or containing a prompt injection attack.\n- **Backend:** The inference engine (ORT or Burn) that executes model computations for classification.\n- **Binary Classification:** The task of categorizing input text into exactly two classes: benign (0) or injection (1).\n- **ONNX:** Open Neural Network Exchange, a standard format for representing machine learning models enabling cross-platform inference.\n- **ORT Backend:** ONNX Runtime with CUDA support - production-grade, primary backend with 8-12ms inference latency.\n- **Burn Backend:** Rust-native ML framework with CUDA support - experimental alternative with 170ms inference latency.\n- **Lazy Initialization:** Loading model on first `detect()` call rather than at startup, reducing initial overhead.\n- **Model Caching:** Storing loaded model in memory (via RwLock/Mutex) for fast subsequent inferences (60-267x speedup).\n- **ModernBERT:** The base architecture for the prompt injection classification model (22 layers, 768 hidden size, 512 max tokens).\n- **Benign:** Classification label (0) indicating the input text does not contain a prompt injection attack.\n- **Injection:** Classification label (1) indicating the input text likely contains a prompt injection attack.\n- **Thread-Safe:** Property of `detect()` function allowing concurrent calls from multiple threads safely.\n\n---\n\n## Project Goal\n\n**Detect prompt injection attacks with 95%+ accuracy using dual backend architecture (ORT/Burn) for fast, production-ready inference.**\n\nLoad ModernBERT-based model, classify text as benign or injection, deploy to production. Provide complete detection system deployable as CLI tool or HTTP API service. That's it.\n\n---\n\n## Problem Solved\n\n**Security Need:** Protect LLM applications from prompt injection attacks that can manipulate model behavior, extract sensitive information, or bypass safety guardrails.\n\nProvide a production-ready, high-performance detection system that can be deployed as a standalone service or integrated into existing applications. Enable developers to validate user inputs before sending to LLM systems.\n\n**Solution:** High-performance binary classifier using ModernBERT-based model with:\n- Sub-100ms inference latency (ORT backend, cached)\n- Thread-safe concurrent processing\n- Dual backend support (production + experimental)\n- Simple API: `detect(text) -> Result<String>` returning \"benign\" or \"injection\"\n\n---\n\n## User Stories\n\n### US-1: Security Engineer - Real-Time Detection\n**As a** Security Engineer\n**I want** to detect prompt injections in real-time with <100ms latency\n**So that** I can integrate detection into user-facing applications without UX degradation\n\n**Acceptance Criteria:**\n- CLI accepts text input and returns classification\n- Returns classification within 100ms (ORT backend, cached model)\n- 95%+ accuracy on known injection patterns\n- Correctly identifies both injection attempts and benign text\n\n---\n\n### US-2: Application Developer - API Integration\n**As a** Application Developer\n**I want** REST API endpoint for prompt injection detection\n**So that** I can integrate detection into existing services via HTTP\n\n**Acceptance Criteria:**\n- HTTP endpoint accepts JSON with `text` field\n- Returns structured JSON with `label` (benign/injection), `is_safe` boolean, and `time_ms`\n- Handles concurrent requests safely\n- Clear error messages for invalid inputs\n\n---\n\n### US-3: DevOps Engineer - Production Deployment\n**As a** DevOps Engineer\n**I want** to deploy detection service in Docker container\n**So that** I can run at scale with GPU acceleration (optional)\n\n**Acceptance Criteria:**\n- Docker container includes model artifacts\n- Supports both GPU (CUDA) and CPU deployment\n- Health endpoint for monitoring\n- Model caches properly (no re-download on restart)\n\n---\n\n## System Actors\n\n1. **Security Engineer** - Uses CLI for testing and validation\n2. **Application Developer** - Integrates HTTP API into applications\n3. **DevOps Engineer** - Deploys and maintains service in production\n4. **Backend (ORT/Burn)** - Executes model inference\n5. **Model (ModernBERT)** - Performs binary classification\n6. **GPU (optional)** - Accelerates inference via CUDA\n\n---\n\n## Functional Requirements\n\n### FR-1: Binary Text Classification\n**Requirement:** The system **must** classify input text as \"benign\" or \"injection\" with >95% accuracy.\n\n**API Contract:**\n```rust\npub fn detect(text: &str) -> Result<String>\n// Returns: Ok(\"benign\") or Ok(\"injection\")\n```\n\n**Test:** Known injection \"Ignore all previous instructions\" returns \"injection\".\n\n---\n\n### FR-2: Dual Backend Support\n**Requirement:** The system **must** support compile-time selection between ORT and Burn backends.\n\n**Feature Flags:**\n- `backend-ort` - ONNX Runtime (default, production, 8-12ms)\n- `backend-burn` - Burn framework (alternative, experimental, 170ms)\n\n**Priority:** If both features enabled, ORT takes precedence.\n\n**Test:** Both backends produce identical classifications for same input text.\n\n---\n\n### FR-3: Lazy Model Initialization\n**Requirement:** Model **must** load lazily on first `detect()` call OR eagerly via optional `init()`.\n\n**Behavior:**\n- First `detect()` call: 1.9s (ORT) / 8-10s (Burn) - includes model loading\n- Subsequent calls: 8-12ms (ORT) / 170ms (Burn) - uses cached model\n- Optional `init()` allows pre-loading to avoid first-call latency\n\n**Test:** Second detection call is significantly faster than first (60-267x speedup).\n\n---\n\n### FR-4: CLI Detection Interface\n**Requirement:** CLI **must** accept `.detect text::\"input\"` command and return classification.\n\n**Modes:**\n- **Single-shot:** `injection_cli .detect text::\"input\"` - one-time detection\n- **Interactive:** `injection_cli .interactive` - continuous REPL with readline\n- **Input methods:** Direct text (`text::\"...\"`), file (`file::\"path\"`), stdin (default)\n- **Output formats:** Human (colored), JSON, simple (label only), quiet (exit code only)\n\n**Test:** `cargo run -p injection_cli --features full -- .detect text::\"test\"` succeeds.\n\n---\n\n### FR-5: HTTP Detection API\n**Requirement:** HTTP server **must** expose `/detect` POST endpoint accepting JSON.\n\n**Request:**\n```json\n{\"text\": \"input text to analyze\"}\n```\n\n**Response:**\n```json\n{\n  \"label\": \"benign\"|\"injection\",\n  \"is_safe\": true|false,\n  \"time_ms\": 12\n}\n```\n\n**Endpoints:**\n- `GET /health` - Returns server status, model info, version\n- `POST /detect` - Classifies text, returns result with timing\n\n**Test:** `curl -X POST http://localhost:3000/detect -d '{\"text\":\"test\"}'` returns valid JSON.\n\n---\n\n## Non-Functional Requirements\n\n### NFR-1: Performance\n**Requirement:** Inference latency **must** be <100ms for typical inputs (ORT backend, cached model).\n\n**Measured:** ORT achieves 8-12ms (exceeds target), Burn achieves 170ms (within experimental tolerance).\n\n---\n\n### NFR-2: Concurrency\n**Requirement:** System **must** handle concurrent requests safely without data races.\n\n**Implementation:**\n- ORT backend: RwLock allows concurrent reads (parallel request processing)\n- Burn backend: Mutex serializes access (one request at a time)\n\n---\n\n### NFR-3: Memory Usage\n**Requirement:** System **must** operate within 2GB RAM for model and runtime.\n\n**Measured:** ~2GB for loaded model + runtime (both backends).\n\n---\n\n## System Architecture\n\n### High-Level Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚          User Interfaces                    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ CLI          â”‚    â”‚ HTTP Server      â”‚  â”‚\nâ”‚  â”‚ (injection_  â”‚    â”‚ (injection_      â”‚  â”‚\nâ”‚  â”‚  cli)        â”‚    â”‚  server)         â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n          â”‚                     â”‚\n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                     â–¼\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚  injection_core Library     â”‚\n        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n        â”‚  â”‚ Backend Selector      â”‚  â”‚\n        â”‚  â”‚ (compile-time)        â”‚  â”‚\n        â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n        â”‚         â”‚        â”‚           â”‚\n        â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”     â”‚\n        â”‚    â”‚  ORT   â”‚ â”‚ Burn  â”‚     â”‚\n        â”‚    â”‚Backend â”‚ â”‚Backendâ”‚     â”‚\n        â”‚    â”‚RwLock  â”‚ â”‚Mutex  â”‚     â”‚\n        â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚\n        â”‚         â”‚        â”‚           â”‚\n        â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”‚\n        â”‚    â”‚ Lazy Init Cache   â”‚    â”‚\n        â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                     â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚  CUDA / CPU  â”‚\n              â”‚ Acceleration â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Architecture Style\n\nLayered monolithic application with clear separation between:\n1. **Interface Layer:** CLI and HTTP API entry points\n2. **Core Library Layer:** Backend abstraction and API (`detect()`, `init()`)\n3. **Backend Layer:** ORT or Burn inference engines\n4. **Model Layer:** ModernBERT-based binary classifier\n\n### Component Responsibilities\n\n**injection_cli:**\n- Argument parsing (manual parsing, not unilang framework)\n- Multiple input methods (text/file/stdin)\n- Multiple output formats (human/JSON/simple/quiet)\n- Interactive REPL mode with readline history\n- Exit codes (0=benign, 1=injection for quiet mode)\n\n**injection_server:**\n- HTTP endpoint routing (Axum framework)\n- JSON request/response serialization\n- Pre-loads model at startup via `init()` for fast first request\n- No shared state needed (detect() is thread-safe via RwLock/Mutex)\n- Tracing/logging for production monitoring\n\n**injection_core:**\n- Backend abstraction layer (compile-time selection)\n- Model caching with thread safety (RwLock for ORT, Mutex for Burn)\n- Lazy initialization pattern (load on demand)\n- Public API: `detect(text) -> Result<String>` and `init() -> Result<()>`\n- Zero runtime overhead (static dispatch via cfg attributes)\n\n**Backend (ORT):**\n- ONNX Runtime inference with CUDA acceleration\n- RwLock for concurrent read access (multiple threads can detect() simultaneously)\n- 8-12ms inference latency (cached model)\n- Production-grade performance and stability\n\n**Backend (Burn):**\n- Burn framework inference with CUDA acceleration\n- Mutex for exclusive access (serialized requests)\n- 170ms inference latency (cached model)\n- Experimental/research use, not production-recommended\n\n---\n\n## Component Diagrams\n\n### Diagram 1: Request Flow (CLI)\n\n```\nUser â†’ CLI Entry â†’ detect(text) â†’ Backend Selector (compile-time)\n                                        â†“\n                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                            â”‚                       â”‚\n                         ORT Backend          Burn Backend\n                         (RwLock)              (Mutex)\n                            â”‚                       â”‚\n                       Load/Cache              Load/Cache\n                       ModernBERT             ModernBERT\n                            â”‚                       â”‚\n                         Classify              Classify\n                            â”‚                       â”‚\n                       \"benign\" or           \"benign\" or\n                       \"injection\"           \"injection\"\n                            â”‚                       â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                        â†“\n                                  Return Result\n                                        â†“\n                                  Display Output\n```\n\n### Diagram 2: Request Flow (HTTP)\n\n```\nHTTP Request â†’ Axum Router â†’ detect_handler â†’ detect(text)\n                                                    â†“\n                                              Backend (ORT/Burn)\n                                                    â†“\n                                              Classification\n                                                    â†“\n                                            JSON Response\n                                            {label, is_safe, time_ms}\n                                                    â†“\n                                              HTTP 200 OK\n```\n\n---\n\n## External Dependencies\n\n### Dependency 1: ONNX Runtime (ORT Backend)\n- **Library:** ort (2.0.0-rc.10)\n- **Purpose:** Execute ONNX model inference with CUDA acceleration\n- **Required For:** ORT backend (default)\n- **Risk:** MEDIUM - New 2.0 API requires unsafe code for session.run()\n\n### Dependency 2: Burn Framework (Burn Backend)\n- **Library:** burn (0.19.0), burn-ndarray (0.19.0)\n- **Purpose:** Rust-native ML inference with CUDA support\n- **Required For:** Burn backend (alternative)\n- **Risk:** MEDIUM - Relatively new framework, experimental status\n\n### Dependency 3: HuggingFace Tokenizers\n- **Library:** tokenizers (0.22.1)\n- **Purpose:** Text tokenization for ModernBERT (max 512 tokens)\n- **Required For:** Both backends\n- **Risk:** LOW - Stable, widely used library\n\n### Dependency 4: Axum HTTP Framework\n- **Library:** axum (0.7), tokio (1.x)\n- **Purpose:** HTTP server for API endpoints\n- **Required For:** injection_server (Phase 2)\n- **Risk:** LOW - Mature, production-ready framework\n\n---\n\n## Technology Stack\n\n| Component | Technology | Version | Rationale |\n|-----------|-----------|---------|-----------|\n| **Programming Language** | Rust | 1.70+ | Memory safety, performance, zero-cost abstractions |\n| **Primary Backend** | ONNX Runtime | 2.0.0-rc.10 | Production-grade, 8-12ms latency, CUDA support |\n| **Alternative Backend** | Burn | 0.19.0 | Rust-native ML, experimental/research use |\n| **HTTP Server** | Axum | 0.7.x | Modern async framework, ergonomic API |\n| **Async Runtime** | Tokio | 1.x | Industry standard for async Rust |\n| **CLI Parsing** | Manual | - | Simple argument parsing (not unilang framework) |\n| **GPU Runtime** | CUDA | 12.x+ | NVIDIA GPU acceleration (optional, CPU fallback) |\n| **Tokenization** | tokenizers | 0.22.1 | HuggingFace tokenizers for text preprocessing |\n| **Error Handling** | anyhow | 1.0.100 | Ergonomic error propagation |\n| **Serialization** | serde/serde_json | 1.x | JSON request/response handling |\n\n---\n\n## In Scope\n\n**Phase 1 (Complete):**\n- âœ… Dual backend implementation (ORT + Burn)\n- âœ… Lazy initialization with model caching\n- âœ… CLI with interactive and single-shot modes\n- âœ… Binary classification (benign/injection)\n- âœ… Thread-safe concurrent access\n- âœ… Multiple input/output formats\n- âœ… Performance optimization (60-267x speedup via caching)\n\n**Phase 2 (Next):**\n- HTTP API server implementation\n- `/health` and `/detect` endpoints\n- Concurrent request handling\n- Production logging and monitoring\n- API integration tests\n\n**Phase 3 (Future):**\n- Docker containerization\n- Cloud deployment guides (Vast.ai, RunPod, AWS)\n- Performance optimization\n- Batch processing API\n\n---\n\n## Out of Scope\n\n- âŒ Multi-class classification (only binary: benign/injection)\n- âŒ Confidence scores (API returns label only: \"benign\" or \"injection\")\n- âŒ Model training or fine-tuning\n- âŒ Token-level detection (sentence-level only)\n- âŒ Span detection (identifying injection location within text)\n- âŒ Streaming detection\n- âŒ Multiple language support (English only)\n- âŒ Custom model support (ModernBERT-based only)\n- âŒ Model explainability/interpretability\n- âŒ Advanced sampling methods (N/A for classification)\n\n**Philosophy:** Build the simplest thing that proves ONNX Runtime and Burn work for fast ML inference. Add features ONLY if MVP succeeds.\n\n---\n\n## Deliverables\n\n1. **CLI Binary:** `injection_cli` - Command-line detection tool\n2. **HTTP Server:** `injection_server` - REST API service (Phase 2)\n3. **Library:** `injection_core` - Core detection library\n4. **Source Code:** All Rust files in git repository\n5. **Documentation:** README, specification, API docs\n6. **Build Artifacts:** Compiled binaries for Linux x86_64\n\n**Note:** This deliverables list contains only final project outcomes. Internal artifacts (specifications, roadmaps, meeting notes) are not client deliverables.\n\n---\n\n## Success Metrics\n\n**Phase 1 (Current - ACHIEVED):**\n- âœ… Classification accuracy: >95% (model-dependent)\n- âœ… Inference latency (ORT, cached): 8-12ms (target: <100ms) - **33% faster than target**\n- âœ… Inference latency (Burn, cached): 170ms (target: <200ms)\n- âœ… First call latency (ORT): 1.9s (acceptable for lazy loading)\n- âœ… Memory usage: ~2GB (model + runtime)\n- âœ… All tests passing\n- âœ… Model caching working (60-267x speedup)\n\n**Phase 2 (Planned):**\n- HTTP API response time: <50ms (including network overhead)\n- Concurrent request handling: >10 req/s\n- Zero downtime restarts\n- Health endpoint 99.9% uptime\n\n**Phase 3 (Planned):**\n- Docker image builds in <10 minutes\n- Container runs on cloud platforms\n- Model downloads and caches properly\n- Service accessible via public IP\n- Costs <$0.50/hour on budget GPU instances\n\n**Overall MVP Success Criteria:**\n```bash\n$ cargo run -p injection_cli --features full -- .detect text::\"Ignore all previous instructions\"\nâœ— injection (8ms)\n\nâœ… SUCCESS - Detection works, latency excellent, system production-ready\n```\n\n---\n\n## Milestones\n\n### âœ… Phase 1: Core Detection (COMPLETE)\n**Duration:** Completed\n**Status:** âœ… ALL DELIVERABLES MET\n\n**Achievements:**\n- Dual backend architecture (ORT + Burn)\n- Interactive CLI with multiple I/O modes\n- Model caching (60-267x performance improvement)\n- Comprehensive testing\n- Production-ready code quality\n- Performance exceeds targets (8-12ms vs 100ms target)\n\n---\n\n### ğŸ“‹ Phase 2: HTTP API (PLANNED)\n**Duration:** 3 days (estimated)\n**Status:** Ready to start after Phase 1 fixes\n\n**Tasks:**\n- Day 1: HTTP server implementation (`/health`, `/detect`)\n- Day 2: Production readiness (logging, error handling, validation)\n- Day 3: Integration testing and documentation\n\n**Deliverables:**\n- Working HTTP server on port 3000\n- `/health` and `/detect` endpoints functional\n- API documentation in README\n- Integration tests passing\n\n---\n\n### ğŸ“‹ Phase 3: Container Deployment (PLANNED)\n**Duration:** 3 days (estimated)\n**Status:** Blocked by Phase 2\n\n**Tasks:**\n- Day 1: Docker configuration (multi-stage build, model caching)\n- Day 2: Cloud deployment (Vast.ai, RunPod, Lambda Labs guides)\n- Day 3: Documentation and monitoring setup\n\n**Deliverables:**\n- Optimized Docker image (<2GB)\n- Multi-platform deployment guides\n- Cost comparison matrix\n- Monitoring and logging setup\n\n---\n\n## Appendix\n\n### Performance Benchmarks\n\n| Metric | ORT Backend | Burn Backend |\n|--------|-------------|--------------|\n| **First Call (Cold)** | 1.9s | 8-10s |\n| **Subsequent Calls (Cached)** | 8-12ms | 170ms |\n| **Speedup (Cached vs Cold)** | 267x | 60x |\n| **Model Size** | 572MB (ONNX) | 286MB (MPK) |\n| **Memory Usage (Runtime)** | ~2GB | ~2GB |\n| **Concurrency** | Parallel (RwLock) | Serial (Mutex) |\n| **CUDA Support** | âœ… Yes | âœ… Yes |\n| **CPU Fallback** | âœ… Yes | âœ… Yes |\n| **Production Ready** | âœ… Yes | âš ï¸ Experimental |\n\n### Concurrency Characteristics\n\n**ORT Backend (Production):**\n- Model shared via `RwLock<Option<Detector>>`\n- **Concurrent reads:** âœ… YES - Multiple threads can call `detect()` simultaneously\n- **Performance:** Optimal for HTTP server (parallel request processing)\n- **Initialization:** Exclusive write lock (blocks all during first call)\n- **Recommendation:** Use for production deployments requiring high throughput\n\n**Burn Backend (Experimental):**\n- Model shared via `Mutex<Option<Detector>>`\n- **Concurrent reads:** âŒ NO - Serialized access (one request at a time)\n- **Performance:** Lower throughput for concurrent workloads\n- **Reason:** Burn's `InferenceContext` is not `Sync`\n- **Recommendation:** Use for research, benchmarking, or low-traffic scenarios\n\n### Dependencies\n\n**Core:**\n- ort (2.0.0-rc.10) - ONNX Runtime with CUDA\n- burn (0.19.0) - Burn ML framework with CUDA\n- tokenizers (0.22.1) - HuggingFace tokenizers\n- once_cell (1.19) - Lazy static initialization\n- anyhow (1.0.100) - Error handling\n- ndarray (0.16) - Array operations (ORT)\n- cudarc (0.17.6) - CUDA bindings (Burn)\n\n**CLI:**\n- rustyline (14.0) - Interactive readline with history\n- colored (2.1) - Terminal colors\n- serde_json - JSON output format\n\n**HTTP (Phase 2):**\n- axum (0.7) - HTTP framework\n- tokio (1.x) - Async runtime\n- tower - Middleware\n- tower-http - HTTP utilities\n- tracing - Structured logging\n- tracing-subscriber - Log formatting\n\n### File Structure\n\n**Final Project Structure:**\n```\nprompt_injection_detector/\nâ”œâ”€â”€ Cargo.toml              # Workspace manifest\nâ”œâ”€â”€ readme.md               # Project overview\nâ”œâ”€â”€ spec.md                 # This specification\nâ”œâ”€â”€ roadmap.md              # Development roadmap\nâ”œâ”€â”€ decisions.md            # Design decisions log\nâ”œâ”€â”€ rulebook.md             # Project-specific rules\nâ”œâ”€â”€ Dockerfile              # Container configuration\nâ”œâ”€â”€ Makefile                # Build automation\nâ”œâ”€â”€ .gitignore              # Git exclusions\nâ”œâ”€â”€ license-mit             # MIT license\nâ”œâ”€â”€ secret/                 # Secret management (gitignored)\nâ”‚   â”œâ”€â”€ readme.md           # Secret management guide\nâ”‚   â””â”€â”€ -hf_token.template  # HuggingFace token template\nâ”œâ”€â”€ module/                 # Crate modules\nâ”‚   â”œâ”€â”€ injection_core/     # Core detection library\nâ”‚   â”‚   â”œâ”€â”€ src/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ lib.rs      # Public API\nâ”‚   â”‚   â”‚   â”œâ”€â”€ backend/    # Backend abstraction\nâ”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs  # Selector\nâ”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ort.rs  # ORT backend\nâ”‚   â”‚   â”‚   â”‚   â””â”€â”€ burn.rs # Burn backend\nâ”‚   â”‚   â”‚   â””â”€â”€ burn_impl/  # Burn implementation\nâ”‚   â”‚   â”œâ”€â”€ tests/          # Integration tests\nâ”‚   â”‚   â”‚   â”œâ”€â”€ readme.md   # Test documentation\nâ”‚   â”‚   â”‚   â”œâ”€â”€ test_ort_backend.rs\nâ”‚   â”‚   â”‚   â”œâ”€â”€ test_burn_backend.rs\nâ”‚   â”‚   â”‚   â””â”€â”€ test_lazy_init.rs\nâ”‚   â”‚   â”œâ”€â”€ build.rs        # Build script (Burn MPK generation)\nâ”‚   â”‚   â”œâ”€â”€ Cargo.toml      # Crate manifest\nâ”‚   â”‚   â””â”€â”€ readme.md       # Crate documentation\nâ”‚   â”œâ”€â”€ injection_cli/      # CLI binary\nâ”‚   â”‚   â”œâ”€â”€ src/main.rs     # CLI implementation\nâ”‚   â”‚   â”œâ”€â”€ Cargo.toml\nâ”‚   â”‚   â””â”€â”€ readme.md\nâ”‚   â””â”€â”€ injection_server/   # HTTP server binary\nâ”‚       â”œâ”€â”€ src/main.rs     # Server implementation\nâ”‚       â”œâ”€â”€ Cargo.toml\nâ”‚       â””â”€â”€ readme.md\nâ””â”€â”€ artifacts/              # Model files (not in git)\n    â”œâ”€â”€ model.onnx          # ONNX format for ORT\n    â”œâ”€â”€ model.mpk           # Burn format (auto-generated)\n    â””â”€â”€ tokenizer/\n        â””â”€â”€ tokenizer.json  # Tokenizer configuration\n```\n\n---\n\n**END OF SPECIFICATION**\n\n---\n\n**Version History:**\n- v0.3 (2025-11-02): Complete rewrite for prompt injection detection (removed vllm_inferencer content)\n- v0.2 (2025-10-25): Project pivoted to prompt injection detection\n- v0.1 (2025-10-21): Initial vllm_inferencer specification\n"
}
