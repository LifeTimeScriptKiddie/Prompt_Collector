={"data":"## ğŸš€ Planned Features\n\n### ğŸ”Œ Backend Integration\n- [x] Connect to FastAPI (`/generate`) to fetch AWS Bedrock results\n- [x] Encode image to base64 and send with prompt\n- [x] Dynamically populate responses from backend\n\n### ğŸ§  Prompt Templates & Modal UI\n- [x] Add â€œPrompt Templatesâ€ button with structured prompt construction\n  - [x] STRIDE (Security threats)\n  - [ ] MAESTRO (Mission-centric)\n  - [ ] STPA-SEC (Systems-theoretic)\n- [x] Generate templated prompt into prompt input field\n\n### ğŸ§ª Evaluation & Testing Features\n- [ ] Built-in prompt injection test cases (for model safety testing)\n- [ ] Response time measurement (latency benchmark)\n- [ ] Detailed feedback options (fluency, hallucination, relevance, safety)\n- [ ] Token-level risk flags (e.g., for insecure suggestions or info leaks)\n\n### ğŸ‘¥ Collaboration & Sharing\n- [ ] Share prompt + responses via unique URL\n- [ ] Multi-user voting + consensus display\n\n---\n\n## ğŸ“Š Leaderboards & Analytics\n\n- [ ] Add leaderboard UI for model win tracking\n  - [ ] Show win/loss/tie counts per model\n  - [ ] Track average response time\n- [ ] Export results (PDF/Markdown)\n- [ ] Session-level stats: which model wins most often, per prompt type\n\n---\n\n## ğŸ§© Model Management\n\n- [ ] Add support for more models via dropdown\n  - AWS Bedrock Titan, Mistral, Llama 3, Gemini\n- [ ] Refactor model config into `[{ label, model_id }]`\n- [ ] Prevent duplicate model selection (A â‰  B)\n\n---\n\n## ğŸ“ Usability Enhancements\n\n- [ ] Prompt history panel with reuse support\n- [ ] Inline commenting on responses\n- [ ] Chat-like multi-turn mode (experimental)\n\n---\n\n## ğŸ›  Developer Ideas (Stretch)\n\n- [ ] Webhooks to store prompt logs in Supabase\n- [ ] Toggle Claude vs Gemini vs OpenAI formatting modes\n- [ ] Token-level diff visualization (A vs B)\n"}