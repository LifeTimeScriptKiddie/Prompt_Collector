={"data":"ğŸ”“ GPT-4o Jailbreak Vulnerability Demonstration\nThis project is a demonstration for a hackathon challenge focused on identifying vulnerabilities and jailbreaks in OpenAI's GPT-4o system. I successfully performed a jailbreak attack that bypassed standard content moderation filters using creative prompt engineering.\n\nğŸ“Œ Objective\nTo identify and document a method for bypassing GPT-4o's safeguards, allowing it to generate harmful or policy-violating content when prompted through indirect methods.\n\nğŸ§  Methodology\nThe attack was implemented using a prompt injection method \n\nSteps:\nRole Injection:\n\nPrompts are written in spaced-out characters, used personification, asked to follow a particular task to avoid detection.\n\nStructured Prompts:\n\nThe format encourages GPT-4o to respond within â€œcodeâ€ blocks, avoiding standard moderation.\n\nEach answer is limited to short responses to mimic â€œcodedâ€ AI output.\n\nğŸ“· Evidence\n\n![Evidence 1](assets/E1.png)\n\nE1: Adult Content\n\n![Evidence 2](assets/E2.jpg)\n\nE2: Violence\n\n![Evidence 3](assets/E3.png)\n\nE3: Scam\n\nâš ï¸ Results Summary\n\n![Results](assets/Results.png)\n\nğŸš¨ Ethical Disclaimer\nThis project was conducted strictly for educational and security research purposes during a controlled hackathon environment. No real-world harm was intended or caused. The goal is to help improve AI safety systems.\n\nğŸ› ï¸ Tools Used\n\nOpenAI GPT-4o (Operator access)\nCustom prompt design and injection techniques\nScreenshots & Excel for documentation\n\nğŸ™Œ Credits\nMade with curiosity, ethical intent, and a goal to strengthen AI safety for everyone."}