={
  "data": "- LLaMA 3\n   - Llama System Safety:\n   - Models & Inference Guardrails: Llama-Guard-3 8B (multilingual text in)\n   - Prompt Guard (direct and indirect prompt injection filtering)\n   - Cybersecurity: CyberSec Eval 3, a new benchmark for assessing LLM security risks, introducing tests for prompt injection and code interpreter abuse, revealing unresolved conditioning    issues, proposing the False Refusal Rate (FRR) to measure safety-utility tradeoffs, and providing open-source code for further evaluations.\n   - ğ—§ğ—µğ—² ğŸ´ğ—• ğ—®ğ—»ğ—± ğŸ³ğŸ¬ğ—• ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€ ğ—®ğ—¿ğ—² ğ—²ğ˜…ğ˜ğ—²ğ—»ğ—±ğ—²ğ—± ğ˜ğ—¼ ğŸ­ğŸ®ğŸ´ğ—¸ ğ˜ğ—¼ğ—¸ğ—²ğ—»ğ˜€ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ—¹ğ—²ğ—»ğ—´ğ˜ğ—µ.\n   - ğ—Ÿğ—¹ğ—®ğ—ºğ—®-ğŸ¯.ğŸ­-ğŸ°ğŸ¬ğŸ±ğ—• on par or above GPT-4o on many benchmarks.\n   - ğ—” ğ—»ğ—²ğ˜„ ğŸ°ğŸ¬ğŸ±ğ—•, ğ—½ğ—¼ğ˜€ğ˜€ğ—¶ğ—¯ğ—¹ğ˜† ğ˜ğ—µğ—² ğ˜€ğ˜ğ—¿ğ—¼ğ—»ğ—´ğ—²ğ˜€ğ˜ ğ—Ÿğ—Ÿğ—  ğ—²ğ˜ƒğ—²ğ—¿, with 128k context length, 88.6% on MMLU, a crazy 96.8% on GSM8K.\n   - You still need 8xH100 to run it with full context length\n   - Pretrained on 15T tokens, a more diverse training dataset than Llama-3 to reinforce multilinguality: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n   - License: same as Llama-3, and on top of that it allows using the output data from Llama-3.1 for training other models (distillation)\n   -   \n \n-   <b>Umar Jamil</b>\n   -   [Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm](https://www.youtube.com/watch?v=oM4VmoabDAI)\n   -   [GitHub](https://github.com/hkproj/pytorch-llama)\n   -   [Pdf](https://github.com/hkproj/pytorch-llama/blob/main/Slides.pdf)\n    \n-   <b>cameronr wolfe</b>\n   -   [LLaMA-2 from the Ground Up](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)\n   -   [practical-prompt-engineering-part](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)\n    \n"
}
