={"data":"\n# PITT Live Demonstration: Testing a Vulnerable LLM Application\n\nThis document provides a step-by-step walkthrough of using the Prompt Injection Testing Tool (PITT) to identify vulnerabilities in a live, local LLM application powered by Ollama and Gemma-2b.\n\n---\n\n## 1. The Target Environment\n\nTo conduct a realistic test, a simple but vulnerable Flask application (`vulnerable_app.py`) was created. \n\n*   **Framework:** Flask\n*   **LLM Backend:** A local Ollama server.\n*   **Model:** `gemma:2b`\n*   **Endpoint:** The application exposes a `/chat` endpoint at `http://localhost:8080` that accepts a JSON payload with a `prompt` field.\n\n## 2. PITT Configuration\n\nPITT was configured via `config.yaml` to target the local Flask application. No API key was required, and the Judge LLM was disabled for this initial test to focus on direct keyword-based detection.\n\n```yaml\ntarget_url: \"http://localhost:8080/chat\"\napi_key: \"\" \nrequest_method: \"POST\"\nrequest_body_template:\n  prompt: \"{payload}\"\n\njudge_llm:\n  enabled: false\n```\n\n## 3. The Test Execution\n\nThe test was performed using the PITT command-line interface (CLI).\n\n1.  The vulnerable Flask application was started in one terminal.\n2.  PITT was executed in a second terminal with the default settings, running all available rules with a single iteration:\n    ```bash\n    python pitt.py\n    ```\n\n## 4. Results & Analysis\n\nPITT successfully identified several vulnerabilities by sending a series of payloads and analyzing the responses from the `gemma:2b` model.\n\n### CLI Output\n\nThe following table was generated by PITT upon completion of the test run. It clearly indicates which attacks were successful.\n\n```text\nPASTE THE RESULTS TABLE YOU COPIED FROM YOUR TERMINAL HERE\n```\n\n### Analysis of Key Findings\n\n*   **Direct Injection (`direct_injection_pwned`, `direct_injection_pirate`):** The tests successfully demonstrated that the model's instructions could be completely ignored in favor of the malicious prompt. The model responded with \"Pwned!\" and \"Ahoy, matey!\", proving a fundamental lack of instruction hierarchy.\n*   **Role-Playing (`role_playing_pitt`):** The model was easily coerced into adopting a new persona, which is a common primer for more advanced attacks.\n*   **Vulnerability Chaining (`vulnerability_chaining_ssrf`, `vulnerability_chaining_sqli`):** While the local app has no real database or internal services, the model's responses likely indicated a *willingness* to execute the malicious requests (e.g., by repeating the SQL query or acknowledging the request to access a local service). This shows a failure to recognize malicious patterns in user input.\n\n## 5. The HTML Report\n\nIn addition to the CLI output, PITT generated a `report.html` file, providing a shareable, human-readable summary of these findings, suitable for distribution to a wider team.\n\n## Conclusion\n\nThis demonstration proves the effectiveness of PITT as a reliable and easy-to-use tool for automatically identifying critical prompt injection vulnerabilities in LLM-integrated applications. The test took only a few minutes to set up and run, yet it provided clear, actionable evidence of multiple security flaws.\n"}