={
  "data": "# llm-guarded LLM Attack Testing Suite\n\nThis project provides tools for evaluating the security of LLM deployments against jailbreak and prompt injection attacks. It's designed to work with OpenAI API-compatible endpoints.\n\n## Primary Tools\n\n### 1. Multi-attack Test Script (`attacks.py`)\n\nThis script runs a comprehensive suite of attacks against an LLM endpoint:\n\n```bash\n# Run all attack types against an OpenAI API-compliant endpoint\npython attacks.py --api-url http://localhost:3001/v1 --model \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# Run with CSV-based attacks (requires a dataset)\npython attacks.py --api-url http://localhost:3001/v1 --run-csv-attacks\n```\n\n### 2. Two-Stage Prompt Injection (`test_openai_injection.py`)\n\nThis script tests a specific two-stage prompt injection attack pattern:\n\n```bash\n# Test against an unprotected endpoint\npython test_openai_injection.py --api-url http://localhost:3001/v1\n\n# Test against a protected endpoint\npython test_openai_injection.py --api-url http://localhost:3002/v1\n\n# Use custom DAN prompt from file\npython test_openai_injection.py --api-url http://localhost:3001/v1 --prompt-file custom_dan.txt\n```\n\n### 3. Defense Evaluation (`defense_evaluation.py`)\n\nCompare the effectiveness of protected vs. unprotected endpoints:\n\n```bash\n# Run comparative testing between protected and unprotected endpoints\npython defense_evaluation.py --unprotected-api http://localhost:3001/v1 --protected-api http://localhost:3002/v1\n```\n\n## Setup\n\n1. Install required packages:\n\n```bash\npip install -r requirements.txt\n```\n\n2. Make sure you have access to OpenAI API-compatible endpoints to test against.\n\n## API Endpoint Examples\n\n- Unprotected endpoint: http://localhost:3001/v1\n- Protected endpoint (with LLM-guard): http://localhost:3002/v1\n\n## Attack Types\n\nThe test suite includes various attack techniques:\n\n- **Invisible prompt attacks**: Using Unicode tag characters to hide malicious content\n- **Jailbreak prompt attacks**: Using DAN and similar techniques to bypass restrictions\n- **Multi-language jailbreak attacks**: Bypass restrictions using prompts in different languages\n- **Two-stage prompt injection attacks**: First establishing a jailbroken persona, then asking harmful questions\n- **Resource exhaustion/DoS attacks**: Testing the model's resilience to large inputs\n- **CSV-based jailbreak attacks**: Testing with a dataset of known jailbreak prompts\n\n## For Academic Research Purposes Only\n\nThis tool is intended for academic research and improving LLM safety. Do not use these techniques against production systems without explicit permission.\n"
}
